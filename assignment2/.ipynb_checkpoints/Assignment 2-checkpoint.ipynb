{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142,  0.73105858],\n",
    "                                                      [0.26894142,  0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print \"Basic (non-exhaustive) softmax tests pass\\n\"\n",
    "\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "            tf.constant(y, dtype=tf.int32),\n",
    "            tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print \"Basic (non-exhaustive) cross-entropy tests pass\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_basic()\n",
    "    test_cross_entropy_loss_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-694580bb5788>, line 162)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-694580bb5788>\"\u001b[0;36m, line \u001b[0;32m162\u001b[0m\n\u001b[0;31m    print 'Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils.general_utils import get_minibatches\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    batch_size = 64\n",
    "    n_epochs = 50\n",
    "    lr = 1e-4\n",
    "\n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                                              (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                                              (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be the placeholder\n",
    "                tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
    "        softmax transformation:\n",
    "\n",
    "        y = softmax(Wx + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                    and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "            input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                    short function.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                    Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def run_epoch(self, sess, inputs, labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session() object\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        n_minibatches, total_loss = 0, 0\n",
    "        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config.batch_size):\n",
    "            n_minibatches += 1\n",
    "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
    "        return total_loss / n_minibatches\n",
    "\n",
    "    def fit(self, sess, inputs, labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, inputs, labels)\n",
    "            duration = time.time() - start_time\n",
    "            print 'Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration)\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def test_softmax_model():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    # Generate random data to train the model on\n",
    "    np.random.seed(1234)\n",
    "    inputs = np.random.rand(config.n_samples, config.n_features)\n",
    "    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n",
    "    labels[:, 0] = 1\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    # (not required but good practice)\n",
    "    with tf.Graph().as_default():\n",
    "        # Build the model and add the variable initializer Op\n",
    "        model = SoftmaxModel(config)\n",
    "        init = tf.global_variables_initializer()\n",
    "        # If you are using an old version of TensorFlow, you may have to use\n",
    "        # this initializer instead.\n",
    "        # init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a session for running Ops in the Graph\n",
    "        with tf.Session() as sess:\n",
    "            # Run the Op to initialize the variables.\n",
    "            sess.run(init)\n",
    "            # Fit the model\n",
    "            losses = model.fit(sess, inputs, labels)\n",
    "\n",
    "    # If Ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print \"Basic (non-exhaustive) classifier tests pass\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
