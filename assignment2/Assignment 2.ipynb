{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    row_max = tf.reduce_max(x, axis = 1, keep_dims = True) \n",
    "    x = x - row_max\n",
    "    out = tf.exp(x)/tf.reduce_sum(tf.exp(x), axis = 1, keep_dims = True)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142,  0.73105858],\n",
    "                                                      [0.26894142,  0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print \"Basic (non-exhaustive) softmax tests pass\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    y =  tf.cast(y, tf.float32)\n",
    "    out = -tf.reduce_sum(tf.multiply(y, tf.log(yhat)))\n",
    "    ### END YOUR CODE\n",
    "   \n",
    "    \n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "            tf.constant(y, dtype=tf.int32),\n",
    "            tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print \"Basic (non-exhaustive) cross-entropy tests pass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "test_cross_entropy_loss_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"Abstracts a Tensorflow graph for a learning task.\n",
    "\n",
    "    We use various Model classes as usual abstractions to encapsulate tensorflow\n",
    "    computational graphs. Each algorithm you will construct in this homework will\n",
    "    inherit from a Model object.\n",
    "    \"\"\"\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Adds placeholder variables to tensorflow computational graph.\n",
    "\n",
    "        Tensorflow uses placeholder variables to represent locations in a\n",
    "        computational graph where data is inserted.  These placeholders are used as\n",
    "        inputs by the rest of the model building and will be fed data during\n",
    "        training.\n",
    "\n",
    "        See for more information:\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for one step of training.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If labels_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Implements the core of the model that transforms a batch of input data into predictions.\n",
    "\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar) output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        sess.run() to train the model. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor (a scalar).\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        \"\"\"Perform one step of gradient descent on the provided batch of data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "            labels_batch: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            loss: loss over the batch (a scalar)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch):\n",
    "        \"\"\"Make predictions for the provided batch of data\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            predictions: np.ndarray of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch)\n",
    "#         print(\"predict_on_batch\")\n",
    "#         print(feed)\n",
    "        predictions = sess.run(self.pred, feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_prediction_op()\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 59.18 (0.019 sec)\n",
      "Epoch 1: loss = 20.32 (0.011 sec)\n",
      "Epoch 2: loss = 10.92 (0.011 sec)\n",
      "Epoch 3: loss = 7.30 (0.013 sec)\n",
      "Epoch 4: loss = 5.44 (0.013 sec)\n",
      "Epoch 5: loss = 4.32 (0.012 sec)\n",
      "Epoch 6: loss = 3.58 (0.011 sec)\n",
      "Epoch 7: loss = 3.05 (0.011 sec)\n",
      "Epoch 8: loss = 2.65 (0.010 sec)\n",
      "Epoch 9: loss = 2.35 (0.010 sec)\n",
      "Epoch 10: loss = 2.11 (0.011 sec)\n",
      "Epoch 11: loss = 1.91 (0.010 sec)\n",
      "Epoch 12: loss = 1.75 (0.010 sec)\n",
      "Epoch 13: loss = 1.61 (0.011 sec)\n",
      "Epoch 14: loss = 1.49 (0.010 sec)\n",
      "Epoch 15: loss = 1.39 (0.010 sec)\n",
      "Epoch 16: loss = 1.30 (0.010 sec)\n",
      "Epoch 17: loss = 1.22 (0.010 sec)\n",
      "Epoch 18: loss = 1.15 (0.010 sec)\n",
      "Epoch 19: loss = 1.09 (0.011 sec)\n",
      "Epoch 20: loss = 1.03 (0.009 sec)\n",
      "Epoch 21: loss = 0.98 (0.009 sec)\n",
      "Epoch 22: loss = 0.94 (0.010 sec)\n",
      "Epoch 23: loss = 0.89 (0.012 sec)\n",
      "Epoch 24: loss = 0.86 (0.010 sec)\n",
      "Epoch 25: loss = 0.82 (0.011 sec)\n",
      "Epoch 26: loss = 0.79 (0.009 sec)\n",
      "Epoch 27: loss = 0.76 (0.010 sec)\n",
      "Epoch 28: loss = 0.73 (0.010 sec)\n",
      "Epoch 29: loss = 0.71 (0.010 sec)\n",
      "Epoch 30: loss = 0.68 (0.010 sec)\n",
      "Epoch 31: loss = 0.66 (0.010 sec)\n",
      "Epoch 32: loss = 0.64 (0.010 sec)\n",
      "Epoch 33: loss = 0.62 (0.010 sec)\n",
      "Epoch 34: loss = 0.60 (0.017 sec)\n",
      "Epoch 35: loss = 0.58 (0.013 sec)\n",
      "Epoch 36: loss = 0.57 (0.017 sec)\n",
      "Epoch 37: loss = 0.55 (0.012 sec)\n",
      "Epoch 38: loss = 0.54 (0.014 sec)\n",
      "Epoch 39: loss = 0.52 (0.013 sec)\n",
      "Epoch 40: loss = 0.51 (0.013 sec)\n",
      "Epoch 41: loss = 0.50 (0.011 sec)\n",
      "Epoch 42: loss = 0.48 (0.014 sec)\n",
      "Epoch 43: loss = 0.47 (0.015 sec)\n",
      "Epoch 44: loss = 0.46 (0.015 sec)\n",
      "Epoch 45: loss = 0.45 (0.019 sec)\n",
      "Epoch 46: loss = 0.44 (0.017 sec)\n",
      "Epoch 47: loss = 0.43 (0.016 sec)\n",
      "Epoch 48: loss = 0.42 (0.016 sec)\n",
      "Epoch 49: loss = 0.41 (0.015 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import get_minibatches\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    batch_size = 64\n",
    "    n_epochs = 50\n",
    "    lr = 1e-4\n",
    "\n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                                              (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                                              (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.float32, (self.config.batch_size, self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, (self.config.batch_size, self.config.n_classes))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be the placeholder\n",
    "                tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        if labels_batch is not None:\n",
    "            feed_dict =  {\n",
    "                self.input_placeholder : inputs_batch,\n",
    "                self.labels_placeholder : labels_batch\n",
    "            }\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                self.input_placeholder : inputs_batch\n",
    "            }\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
    "        softmax transformation:\n",
    "\n",
    "        y = softmax(Wx + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                    and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "            input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        W = tf.get_variable(name = \"W\", shape = [self.config.n_features, self.config.n_classes], initializer = tf.constant_initializer(0.0))\n",
    "        b = tf.get_variable(name = \"b\", shape = [self.config.n_classes], dtype = tf.float32, initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        pred = softmax(tf.matmul(self.input_placeholder, W) + b)\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                    short function.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                    Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def run_epoch(self, sess, inputs, labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session() object\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        n_minibatches, total_loss = 0, 0\n",
    "        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config.batch_size):\n",
    "            n_minibatches += 1\n",
    "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
    "        return total_loss / n_minibatches\n",
    "\n",
    "    def fit(self, sess, inputs, labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, inputs, labels)\n",
    "            duration = time.time() - start_time\n",
    "            print 'Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration)\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def test_softmax_model():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    # Generate random data to train the model on\n",
    "    np.random.seed(1234)\n",
    "    inputs = np.random.rand(config.n_samples, config.n_features)\n",
    "    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n",
    "    labels[:, 0] = 1\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    # (not required but good practice)\n",
    "    with tf.Graph().as_default():\n",
    "        # Build the model and add the variable initializer Op\n",
    "        model = SoftmaxModel(config)\n",
    "        init = tf.global_variables_initializer()\n",
    "        # If you are using an old version of TensorFlow, you may have to use\n",
    "        # this initializer instead.\n",
    "        # init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a session for running Ops in the Graph\n",
    "        with tf.Session() as sess:\n",
    "            # Run the Op to initialize the variables.\n",
    "            sess.run(init)\n",
    "            # Fit the model\n",
    "            losses = model.fit(sess, inputs, labels)\n",
    "\n",
    "    # If Ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print \"Basic (non-exhaustive) classifier tests pass\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        Your code should initialize the following fields:\n",
    "            self.stack: The current stack represented as a list with the top of the stack as the\n",
    "                        last element of the list.\n",
    "            self.buffer: The current buffer represented as a list with the first item on the\n",
    "                         buffer as the first item of the list\n",
    "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "                    tuples where each tuple is of the form (head, dependent).\n",
    "                    Order for this list doesn't matter.\n",
    "\n",
    "        The root token should be represented with the string \"ROOT\"\n",
    "\n",
    "        Args:\n",
    "            sentence: The sentence to be parsed as a list of words.\n",
    "                      Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        self.stack = [\"ROOT\"]\n",
    "        self.buffer = sentence[:]\n",
    "        self.dependencies = []\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        Args:\n",
    "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
    "                        and right-arc transitions.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE \n",
    "#         print\n",
    "#         print(\"stack\")\n",
    "#         print(self.stack)\n",
    "#         print(\"buffer\")\n",
    "#         print(self.buffer)\n",
    "#         print(\"dependencies\")\n",
    "#         print(self.dependencies)\n",
    "#         print(\"transition\")\n",
    "#         print(transition)\n",
    "        if transition == \"S\":\n",
    "            word = self.buffer.pop(0)\n",
    "            self.stack.append(word)\n",
    "        elif transition == \"LA\":\n",
    "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
    "            self.stack.pop(-2)\n",
    "\n",
    "#             second_to_last_word = self.stack.pop(-2)\n",
    "#             self.dependencies.append((self.stack[-1], second_to_last_word))\n",
    "        else:\n",
    "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
    "            self.stack.pop(-1)\n",
    "\n",
    "#             last_word = self.stack.pop()\n",
    "#             self.dependencies.append((self.stack[-1], (last_word)))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        Args:\n",
    "            transitions: The list of transitions in the order they should be applied\n",
    "        Returns:\n",
    "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
    "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
    "        model: The model that makes parsing decisions. It is assumed to have a function\n",
    "               model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "               returns a list of transitions predicted for each parse. That is, after calling\n",
    "                   transitions = model.predict(partial_parses)\n",
    "               transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "        batch_size: The number of PartialParses to include in each minibatch\n",
    "    Returns:\n",
    "        dependencies: A list where each element is the dependencies list for a parsed sentence.\n",
    "                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n",
    "                      contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    partial_parses = []\n",
    "    for sentence in sentences:\n",
    "        partial_parses.append(PartialParse(sentence))\n",
    "    unfinished_parses = partial_parses[:]\n",
    "    while len(unfinished_parses) > 0:\n",
    "        parse_batch = unfinished_parses[:min(len(unfinished_parses), batch_size)]\n",
    "        transitions = model.predict(parse_batch) \n",
    "        \n",
    "        for i, pp in enumerate(parse_batch):\n",
    "            pp.parse_step(transitions[i])\n",
    "            if len(pp.buffer) == 0 and len(pp.stack) == 1:\n",
    "                unfinished_parses.remove(pp)\n",
    "        \n",
    "    \n",
    "    dependencies = []\n",
    "    for pp in partial_parses:\n",
    "        dependencies.append(pp.dependencies) \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n",
      "minibatch_parse test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "    print \"{:} test passed!\".format(name)\n",
    "\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print \"parse test passed!\"\n",
    "\n",
    "\n",
    "class DummyModel:\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "    print \"minibatch_parse test passed!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_parse_step()\n",
    "    test_parse()\n",
    "    test_minibatch_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "Basic (non-exhaustive) Xavier initialization tests pass\n"
     ]
    }
   ],
   "source": [
    "def xavier_weight_init():\n",
    "    \"\"\"Returns function that creates random tensor.\n",
    "\n",
    "    The specified function will take in a shape (tuple or 1-d array) and\n",
    "    returns a random tensor of the specified shape drawn from the\n",
    "    Xavier initialization distribution.\n",
    "\n",
    "    Hint: You might find tf.random_uniform useful.\n",
    "    \"\"\"\n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"Defines an initializer for the Xavier distribution.\n",
    "        Specifically, the output should be sampled uniformly from [-epsilon, epsilon] where\n",
    "            epsilon = sqrt(6) / <sum of the sizes of shape's dimensions>\n",
    "        e.g., if shape = (2, 3), epsilon = sqrt(6 / (2 + 3))\n",
    "\n",
    "        This function will be used as a variable initializer.\n",
    "\n",
    "        Args:\n",
    "            shape: Tuple or 1-d array that species the dimensions of the requested tensor.\n",
    "        Returns:\n",
    "            out: tf.Tensor of specified shape sampled from the Xavier distribution.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        denom = sum(shape)\n",
    "        eps = np.sqrt(6)/np.sqrt(denom)\n",
    "        out = tf.random_uniform(shape, minval = -eps, maxval = eps)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "    # Returns defined initializer function.\n",
    "    return _xavier_initializer\n",
    "\n",
    "def test_initialization_basic():\n",
    "    \"\"\"Some simple tests for the initialization.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    xavier_initializer = xavier_weight_init()\n",
    "    shape = (1,)\n",
    "    xavier_mat = xavier_initializer(shape)\n",
    "    assert xavier_mat.get_shape() == shape\n",
    "\n",
    "    shape = (1, 2, 3)\n",
    "    xavier_mat = xavier_initializer(shape)\n",
    "    assert xavier_mat.get_shape() == shape\n",
    "    print \"Basic (non-exhaustive) Xavier initialization tests pass\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_initialization_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for training the dependency parser.\n",
    "You do not need to read/understand this code\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "P_PREFIX = '<p>:'\n",
    "L_PREFIX = '<l>:'\n",
    "UNK = '<UNK>'\n",
    "NULL = '<NULL>'\n",
    "ROOT = '<ROOT>'\n",
    "\n",
    "\n",
    "class ConfigUtils(object):\n",
    "    language = 'english'\n",
    "    with_punct = True\n",
    "    unlabeled = True\n",
    "    lowercase = True\n",
    "    use_pos = True\n",
    "    use_dep = True\n",
    "    use_dep = use_dep and (not unlabeled)\n",
    "    data_path = './data'\n",
    "    train_file = 'train.conll'\n",
    "    dev_file = 'dev.conll'\n",
    "    test_file = 'test.conll'\n",
    "    embedding_file = './data/en-cw.txt'\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        root_labels = list([l for ex in dataset\n",
    "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
    "        counter = Counter(root_labels)\n",
    "        if len(counter) > 1:\n",
    "            logging.info('Warning: more than one root label')\n",
    "            logging.info(counter)\n",
    "        self.root_label = counter.most_common()[0][0]\n",
    "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
    "                                               for w in ex['label']\n",
    "                                               if w != self.root_label]))\n",
    "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
    "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
    "\n",
    "        config = ConfigUtils()\n",
    "        self.unlabeled = config.unlabeled\n",
    "        self.with_punct = config.with_punct\n",
    "        self.use_pos = config.use_pos\n",
    "        self.use_dep = config.use_dep\n",
    "        self.language = config.language\n",
    "\n",
    "        if self.unlabeled:\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1\n",
    "        else:\n",
    "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
    "            self.n_deprel = len(deprel)\n",
    "\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        # logging.info('Build dictionary for part-of-speech tags.')\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "\n",
    "        # logging.info('Build dictionary for words.')\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK] = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "\n",
    "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
    "        self.n_tokens = len(tok2id)\n",
    "\n",
    "    def vectorize(self, examples):\n",
    "        vec_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['label']]\n",
    "            vec_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "        return vec_examples\n",
    "\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0\n",
    "\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = []\n",
    "        l_features = []\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        if self.use_pos:\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "\n",
    "        for i in xrange(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1]\n",
    "                lc = get_lc(k)\n",
    "                rc = get_rc(k)\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.use_pos:\n",
    "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.use_dep:\n",
    "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
    "            else:\n",
    "                features += [self.NULL] * 6\n",
    "                if self.use_pos:\n",
    "                    p_features += [self.P_NULL] * 6\n",
    "                if self.use_dep:\n",
    "                    l_features += [self.L_NULL] * 6\n",
    "\n",
    "        features += p_features + l_features\n",
    "        assert len(features) == self.n_features\n",
    "        return features\n",
    "\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "\n",
    "        i0 = stack[-1]\n",
    "        i1 = stack[-2]\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        l0 = ex['label'][i0]\n",
    "        l1 = ex['label'][i1]\n",
    "\n",
    "        if self.unlabeled:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return 0\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return 1\n",
    "            else:\n",
    "                return None if len(buf) == 0 else 2\n",
    "        else:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
    "            else:\n",
    "                return None if len(buf) == 0 else self.n_trans - 1\n",
    "\n",
    "    def create_instances(self, examples):\n",
    "        all_instances = []\n",
    "        succ = 0\n",
    "        for id, ex in enumerate(logged_loop(examples)):\n",
    "            n_words = len(ex['word']) - 1\n",
    "\n",
    "            # arcs = {(h, t, label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in xrange(n_words)]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            for i in xrange(n_words * 2):\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                legal_labels = self.legal_labels(stack, buf)\n",
    "                assert legal_labels[gold_t] == 1\n",
    "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
    "                                  legal_labels, gold_t))\n",
    "                if gold_t == self.n_trans - 1:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                elif gold_t < self.n_deprel:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                succ += 1\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
    "        labels += [1] if len(buf) > 0 else [0]\n",
    "        return labels\n",
    "\n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        for i, example in enumerate(dataset):\n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]\n",
    "            sentences.append(sentence)\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "\n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "\n",
    "        UAS = all_tokens = 0.0\n",
    "        for i, ex in enumerate(dataset):\n",
    "            head = [-1] * len(ex['word'])\n",
    "            for h, t, in dependencies[i]:\n",
    "                head[t] = h\n",
    "            for pred_h, gold_h, gold_l, pos in \\\n",
    "                    zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
    "                    assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                    pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                    if (self.with_punct) or (not punct(self.language, pos_str)):\n",
    "                        UAS += 1 if pred_h == gold_h else 0\n",
    "                        all_tokens += 1\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies\n",
    "\n",
    "\n",
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "#         print(\"mb_x\")\n",
    "#         print(mb_x)\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "#         print(\"mb_l\")\n",
    "#         print(mb_l)\n",
    "        pred = self.parser.model.predict_on_batch(self.parser.session, mb_x)\n",
    "#         print(\"model output is:\")\n",
    "#         print(pred) \n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "#         print(\"pred is:\")\n",
    "#         print(pred)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "#         print(\"transition is:\")\n",
    "#         print(pred)\n",
    "        return pred\n",
    "\n",
    "\n",
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    examples = []\n",
    "    with open(in_file) as f:\n",
    "        word, pos, head, label = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split('\\t')\n",
    "            if len(sp) == 10:\n",
    "                if '-' not in sp[0]:\n",
    "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
    "                    pos.append(sp[4])\n",
    "                    head.append(int(sp[6]))\n",
    "                    label.append(sp[7])\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "                word, pos, head, label = [], [], [], []\n",
    "                if (max_example is not None) and (len(examples) == max_example):\n",
    "                    break\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_dict(keys, n_max=None, offset=0):\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    ls = count.most_common() if n_max is None \\\n",
    "        else count.most_common(n_max)\n",
    "\n",
    "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
    "\n",
    "\n",
    "def punct(language, pos):\n",
    "    if language == 'english':\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    elif language == 'chinese':\n",
    "        return pos == 'PU'\n",
    "    elif language == 'french':\n",
    "        return pos == 'PUNC'\n",
    "    elif language == 'german':\n",
    "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
    "    elif language == 'spanish':\n",
    "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
    "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
    "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
    "                       \"fx\", \"fz\"]\n",
    "    elif language == 'universal':\n",
    "        return pos == 'PUNCT'\n",
    "    else:\n",
    "        raise ValueError('language: %s is not supported.' % language)\n",
    "\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(reduced=True):\n",
    "    config = ConfigUtils()\n",
    "\n",
    "    print \"Loading data...\",\n",
    "    start = time.time()\n",
    "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
    "                           lowercase=config.lowercase)\n",
    "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
    "                         lowercase=config.lowercase)\n",
    "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
    "                          lowercase=config.lowercase)\n",
    "    if reduced:\n",
    "        train_set = train_set[:1000]\n",
    "        dev_set = dev_set[:500]\n",
    "        test_set = test_set[:500]\n",
    "    print \"took {:.2f} seconds\".format(time.time() - start)\n",
    "\n",
    "    print \"Building parser...\",\n",
    "    start = time.time()\n",
    "    parser = Parser(train_set)\n",
    "    print \"took {:.2f} seconds\".format(time.time() - start)\n",
    "\n",
    "    print \"Loading pretrained embeddings...\",\n",
    "    start = time.time()\n",
    "    word_vectors = {}\n",
    "    for line in open(config.embedding_file).readlines():\n",
    "        sp = line.strip().split()\n",
    "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "    for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "    print \"took {:.2f} seconds\".format(time.time() - start)\n",
    "\n",
    "    print \"Vectorizing data...\",\n",
    "    start = time.time()\n",
    "    train_set = parser.vectorize(train_set)\n",
    "    dev_set = parser.vectorize(dev_set)\n",
    "    test_set = parser.vectorize(test_set)\n",
    "    print \"took {:.2f} seconds\".format(time.time() - start)\n",
    "\n",
    "    print \"Preprocessing training data...\"\n",
    "    train_examples = parser.create_instances(train_set)\n",
    "\n",
    "    return parser, embeddings_matrix, train_examples, dev_set, test_set,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
    "    iterate through data in minibatches as follows:\n",
    "\n",
    "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Or with multiple data sources:\n",
    "\n",
    "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Args:\n",
    "        data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "        minibatch_size: the maximum number of items in a minibatch\n",
    "        shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
    "              list. This can be used to iterate through multiple data sources\n",
    "              (e.g., features and labels) at the same time.\n",
    "\n",
    "    \"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else minibatch(data, minibatch_indices)\n",
    "\n",
    "\n",
    "def minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "\n",
    "def test_all_close(name, actual, expected):\n",
    "    if actual.shape != expected.shape:\n",
    "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
    "                         .format(name, expected.shape, actual.shape))\n",
    "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
    "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
    "    else:\n",
    "        print name, \"passed!\"\n",
    "\n",
    "\n",
    "def logged_loop(iterable, n=None):\n",
    "    if n is None:\n",
    "        n = len(iterable)\n",
    "    step = max(1, n / 1000)\n",
    "    prog = Progbar(n)\n",
    "    for i, elem in enumerate(iterable):\n",
    "        if i % step == 0 or i == n - 1:\n",
    "            prog.update(i + 1)\n",
    "        yield elem\n",
    "\n",
    "\n",
    "class Progbar(object):\n",
    "    \"\"\"\n",
    "    Progbar class copied from keras (https://github.com/fchollet/keras/)\n",
    "    Displays a progress bar.\n",
    "    # Arguments\n",
    "        target: Total number of steps expected.\n",
    "        interval: Minimum visual progress update interval (in seconds).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, width=30, verbose=1):\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.sum_values = {}\n",
    "        self.unique_values = []\n",
    "        self.start = time.time()\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def update(self, current, values=[], exact=[]):\n",
    "        \"\"\"\n",
    "        Updates the progress bar.\n",
    "        # Arguments\n",
    "            current: Index of current step.\n",
    "            values: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display averages for these values.\n",
    "            exact: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display these values directly.\n",
    "        \"\"\"\n",
    "\n",
    "        for k, v in values:\n",
    "            if k not in self.sum_values:\n",
    "                self.sum_values[k] = [v * (current - self.seen_so_far), current - self.seen_so_far]\n",
    "                self.unique_values.append(k)\n",
    "            else:\n",
    "                self.sum_values[k][0] += v * (current - self.seen_so_far)\n",
    "                self.sum_values[k][1] += (current - self.seen_so_far)\n",
    "        for k, v in exact:\n",
    "            if k not in self.sum_values:\n",
    "                self.unique_values.append(k)\n",
    "            self.sum_values[k] = [v, 1]\n",
    "        self.seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        if self.verbose == 1:\n",
    "            prev_total_width = self.total_width\n",
    "            sys.stdout.write(\"\\b\" * prev_total_width)\n",
    "            sys.stdout.write(\"\\r\")\n",
    "\n",
    "            numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n",
    "            bar = barstr % (current, self.target)\n",
    "            prog = float(current)/self.target\n",
    "            prog_width = int(self.width*prog)\n",
    "            if prog_width > 0:\n",
    "                bar += ('='*(prog_width-1))\n",
    "                if current < self.target:\n",
    "                    bar += '>'\n",
    "                else:\n",
    "                    bar += '='\n",
    "            bar += ('.'*(self.width-prog_width))\n",
    "            bar += ']'\n",
    "            sys.stdout.write(bar)\n",
    "            self.total_width = len(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self.start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            eta = time_per_unit*(self.target - current)\n",
    "            info = ''\n",
    "            if current < self.target:\n",
    "                info += ' - ETA: %ds' % eta\n",
    "            else:\n",
    "                info += ' - %ds' % (now - self.start)\n",
    "            for k in self.unique_values:\n",
    "                if type(self.sum_values[k]) is list:\n",
    "                    info += ' - %s: %.4f' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
    "                else:\n",
    "                    info += ' - %s: %s' % (k, self.sum_values[k])\n",
    "\n",
    "            self.total_width += len(info)\n",
    "            if prev_total_width > self.total_width:\n",
    "                info += ((prev_total_width-self.total_width) * \" \")\n",
    "\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if current >= self.target:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            if current >= self.target:\n",
    "                info = '%ds' % (now - self.start)\n",
    "                for k in self.unique_values:\n",
    "                    info += ' - %s: %.4f' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
    "                sys.stdout.write(info + \"\\n\")\n",
    "\n",
    "    def add(self, n, values=[]):\n",
    "        self.update(self.seen_so_far+n, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data... took 2.91 seconds\n",
      "Building parser... took 0.04 seconds\n",
      "Loading pretrained embeddings... took 2.58 seconds\n",
      "Vectorizing data... took 0.06 seconds\n",
      "Preprocessing training data...\n",
      "1000/1000 [==============================] - 3s     \n",
      "Building model... HUIHIHI\n",
      "Tensor(\"Placeholder:0\", shape=(?, 36), dtype=int32)\n",
      "took 0.31 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n",
      "(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      " 1/24 [>.............................] - ETA: 5s - train loss: 1.4724(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      " 2/24 [=>............................] - ETA: 4s - train loss: 1.7762(2048, 36)\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      " 3/24 [==>...........................] - ETA: 3s - train loss: 1.8336(2048, 36)\n",
      "[[ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      " 4/24 [====>.........................] - ETA: 3s - train loss: 1.6755(2048, 36)\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      " 5/24 [=====>........................] - ETA: 3s - train loss: 1.5075(2048, 36)\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      " 6/24 [======>.......................] - ETA: 3s - train loss: 1.4324(2048, 36)\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      " 7/24 [=======>......................] - ETA: 3s - train loss: 1.3790(2048, 36)\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      " 8/24 [=========>....................] - ETA: 2s - train loss: 1.3166(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n",
      " 9/24 [==========>...................] - ETA: 2s - train loss: 1.2461(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "10/24 [===========>..................] - ETA: 2s - train loss: 1.1846(2048, 36)\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      "11/24 [============>.................] - ETA: 2s - train loss: 1.1334(2048, 36)\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "12/24 [==============>...............] - ETA: 2s - train loss: 1.0935(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "13/24 [===============>..............] - ETA: 1s - train loss: 1.0591(2048, 36)\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "14/24 [================>.............] - ETA: 1s - train loss: 1.0292(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "15/24 [=================>............] - ETA: 1s - train loss: 1.0002(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      "16/24 [===================>..........] - ETA: 1s - train loss: 0.9741(2048, 36)\n",
      "[[ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "17/24 [====================>.........] - ETA: 1s - train loss: 0.9489(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "18/24 [=====================>........] - ETA: 1s - train loss: 0.9244(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n",
      "19/24 [======================>.......] - ETA: 0s - train loss: 0.9023(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "20/24 [========================>.....] - ETA: 0s - train loss: 0.8833(2048, 36)\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "21/24 [=========================>....] - ETA: 0s - train loss: 0.8650(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      "22/24 [==========================>...] - ETA: 0s - train loss: 0.8489(2048, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "23/24 [===========================>..] - ETA: 0s - train loss: 0.8341(1286, 36)\n",
      "[[ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "24/24 [==============================] - 4s - train loss: 0.8193     \n",
      "Evaluating on dev set"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-95dfe8235619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-95dfe8235619>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(debug)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"TRAINING\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;36m80\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-95dfe8235619>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, sess, saver, parser, train_examples, dev_set)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Epoch {:} out of {:}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mdev_UAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdev_UAS\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_dev_UAS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mbest_dev_UAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_UAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-95dfe8235619>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, sess, parser, train_examples, dev_set)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Evaluating on dev set\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mdev_UAS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"- dev UAS: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_UAS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdev_UAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-95aff35c9e58>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, dataset, eval_batch_size)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_id_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mUAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f1d3c4f53678>\u001b[0m in \u001b[0;36mminibatch_parse\u001b[0;34m(sentences, model, batch_size)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfinished_parses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mparse_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfinished_parses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfinished_parses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-95aff35c9e58>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, partial_parses)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;31m#         print(\"mb_l\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;31m#         print(mb_l)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;31m#         print(\"model output is:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;31m#         print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-542a8db675bc>\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, sess, inputs_batch)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m#         print(\"predict_on_batch\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#         print(feed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinlu/miniconda3/envs/cs224n/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinlu/miniconda3/envs/cs224n/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinlu/miniconda3/envs/cs224n/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kevinlu/miniconda3/envs/cs224n/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinlu/miniconda3/envs/cs224n/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import cPickle\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_features = 36\n",
    "    n_classes = 3\n",
    "    dropout = 0.5\n",
    "    embed_size = 50\n",
    "    hidden_size = 200\n",
    "    batch_size = 2048\n",
    "    n_epochs = 10\n",
    "    lr = 0.001\n",
    "\n",
    "\n",
    "class ParserModel(Model):\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network with an embedding layer and single hidden layer.\n",
    "    This network will predict which transition should be applied to a given partial parse\n",
    "    configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
    "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
    "        (so we can use different batch sizes without rebuilding the model).\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of  shape (None, n_features), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape (None, n_classes), type tf.float32\n",
    "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "            self.dropout_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.int32, shape = (None, self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, shape = (None, self.config.n_classes))\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1):\n",
    "        \"\"\"Creates the feed_dict for the dependency parser.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "        Hint: When an argument is None, don't add it to the feed_dict.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "            dropout: The dropout rate.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        if labels_batch is None:\n",
    "            feed_dict = {\n",
    "                self.input_placeholder : inputs_batch,\n",
    "                self.dropout_placeholder : dropout\n",
    "            }\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                self.input_placeholder : inputs_batch,\n",
    "                self.labels_placeholder : labels_batch,\n",
    "                self.dropout_placeholder : dropout\n",
    "            }\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
    "        concatenates those vectors:\n",
    "            - Creates an embedding tensor and initializes it with self.pretrained_embeddings.\n",
    "            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n",
    "              tensor of shape (None, n_features, embedding_size).\n",
    "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
    "              (None, n_features * embedding_size).\n",
    "\n",
    "        Hint: You might find tf.nn.embedding_lookup useful.\n",
    "        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n",
    "            what -1 in a shape means.\n",
    "            https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n",
    "\n",
    "        Returns:\n",
    "            embeddings: tf.Tensor of shape (None, n_features*embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        print(\"HUIHIHI\")\n",
    "        print(self.input_placeholder)\n",
    "        embeddings = tf.nn.embedding_lookup(self.pretrained_embeddings, self.input_placeholder)\n",
    "        embeddings = tf.reshape(embeddings, (-1, self.config.n_features * self.config.embed_size))\n",
    "#         vembeddings = tf.Variable(self.pretrained_embeddings)\n",
    "#         lembeddings = tf.nn.embedding_lookup(vembeddings, self.input_placeholder)\n",
    "#         embeddings = tf.reshape( lembeddings, (-1, self.config.n_features*self.config.embed_size))\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return embeddings\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the 1-hidden-layer NN:\n",
    "            h = Relu(xW + b1)\n",
    "            h_drop = Dropout(h, dropout_rate)\n",
    "            pred = h_drop * U + b2\n",
    "\n",
    "        Note that we are not applying a softmax to pred. The softmax will instead be done in\n",
    "        the add_loss_op function, which improves efficiency because we can use\n",
    "        tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "        Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n",
    "        and b2 with zeros)\n",
    "\n",
    "        Hint: Here are the dimensions of the various variables you will need to create\n",
    "                    W:  (n_features*embed_size, hidden_size)\n",
    "                    b1: (hidden_size,)\n",
    "                    U:  (hidden_size, n_classes)\n",
    "                    b2: (n_classes)\n",
    "        Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument. \n",
    "            The keep probability should be set to the value of self.dropout_placeholder\n",
    "\n",
    "        Returns:\n",
    "            pred: tf.Tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.add_embedding()\n",
    "        ### YOUR CODE HERE\n",
    "        xavier_initializer = xavier_weight_init()\n",
    "        n_features = self.config.n_features\n",
    "        n_classes = self.config.n_classes\n",
    "        embed_size = self.config.embed_size\n",
    "        hidden_size = self.config.hidden_size\n",
    "        dropout = self.dropout_placeholder\n",
    "        \n",
    "        \n",
    "        W_ = tf.Variable(xavier_initializer((n_features * embed_size, hidden_size)))\n",
    "        W = tf.get_variable(name = \"W\", initializer = W_.initialized_value()) \n",
    "        b1 = tf.get_variable(name = \"b1\", shape = (hidden_size,), dtype = tf.float32, initializer = tf.constant_initializer(0.0))\n",
    "        U_ = tf.Variable(xavier_initializer((hidden_size, n_classes)))\n",
    "        U = tf.get_variable(name = \"U\", initializer = U_.initialized_value())\n",
    "        b2 = tf.get_variable(name = \"b2\", shape = (n_classes,), dtype = tf.float32, initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        h = tf.nn.relu(tf.matmul(x, W )+ b1)\n",
    "        h_drop = tf.nn.dropout(h, dropout)\n",
    "        pred = tf.matmul(h_drop, U) + b2\n",
    "        \n",
    "#         xavier_initializer = xavier_weight_init()\n",
    "#         W  = tf.Variable(xavier_initializer((self.config.n_features*self.config.embed_size, self.config.hidden_size)))\n",
    "#         b1 = tf.Variable(tf.zeros((1, self.config.hidden_size)))\n",
    "#         U  = tf.Variable(xavier_initializer((self.config.hidden_size, self.config.n_classes)))\n",
    "#         b2 = tf.Variable(tf.zeros((1, self.config.n_classes)))\n",
    "\n",
    "#         h = tf.nn.relu( tf.matmul(x, W) + b1)\n",
    "#         h_drop = tf.nn.dropout(h, self.dropout_placeholder)\n",
    "#         pred = tf.matmul(h_drop,U) + b2\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "        In this case we are using cross entropy loss.\n",
    "        The loss should be averaged over all examples in the current minibatch.\n",
    "\n",
    "        Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n",
    "                    implementation. You might find tf.reduce_mean useful.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n",
    "                  network before the softmax layer.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.labels_placeholder, logits= pred))\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Use tf.train.AdamOptimizer for this model.\n",
    "        Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
    "                                     dropout=self.config.dropout)\n",
    "#         print \"train_on_batch\"\n",
    "#         print(feed)\n",
    "        \n",
    "        \n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def run_epoch(self, sess, parser, train_examples, dev_set):\n",
    "        prog = Progbar(target=1 + len(train_examples) / self.config.batch_size)\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):\n",
    "            print(train_x.shape)\n",
    "            print(train_y)\n",
    "            loss = self.train_on_batch(sess, train_x, train_y)\n",
    "            prog.update(i + 1, [(\"train loss\", loss)])\n",
    "\n",
    "        print \"Evaluating on dev set\",\n",
    "        dev_UAS, _ = parser.parse(dev_set)\n",
    "        print \"- dev UAS: {:.2f}\".format(dev_UAS * 100.0)\n",
    "        return dev_UAS\n",
    "\n",
    "    def fit(self, sess, saver, parser, train_examples, dev_set):\n",
    "        best_dev_UAS = 0\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            print \"Epoch {:} out of {:}\".format(epoch + 1, self.config.n_epochs)\n",
    "            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n",
    "            if dev_UAS > best_dev_UAS:\n",
    "                best_dev_UAS = dev_UAS\n",
    "                if saver:\n",
    "                    print \"New best dev UAS! Saving model in ./data/weights/parser.weights\"\n",
    "                    saver.save(sess, './data/weights/parser.weights')\n",
    "            print\n",
    "\n",
    "    def __init__(self, config, pretrained_embeddings):\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def main(debug=True):\n",
    "    print 80 * \"=\"\n",
    "    print \"INITIALIZING\"\n",
    "    print 80 * \"=\"\n",
    "    config = Config()\n",
    "    parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
    "    if not os.path.exists('./data/weights/'):\n",
    "        os.makedirs('./data/weights/')\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        print \"Building model...\",\n",
    "        start = time.time()\n",
    "        model = ParserModel(config, embeddings)\n",
    "        parser.model = model\n",
    "        print \"took {:.2f} seconds\\n\".format(time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        # If you are using an old version of TensorFlow, you may have to use\n",
    "        # this initializer instead.\n",
    "        # init = tf.initialize_all_variables()\n",
    "        saver = None if debug else tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            parser.session = session\n",
    "            session.run(init)\n",
    "\n",
    "            print 80 * \"=\"\n",
    "            print \"TRAINING\"\n",
    "            print 80 * \"=\"\n",
    "            model.fit(session, saver, parser, train_examples, dev_set)\n",
    "\n",
    "            if not debug:\n",
    "                print 80 * \"=\"\n",
    "                print \"TESTING\"\n",
    "                print 80 * \"=\"\n",
    "                print \"Restoring the best model weights found on the dev set\"\n",
    "                saver.restore(session, './data/weights/parser.weights')\n",
    "                print \"Final evaluation on test set\",\n",
    "                UAS, dependencies = parser.parse(test_set)\n",
    "                print \"- test UAS: {:.2f}\".format(UAS * 100.0)\n",
    "                print \"Writing predictions\"\n",
    "                with open('q2_test.predicted.pkl', 'w') as f:\n",
    "                    cPickle.dump(dependencies, f, -1)\n",
    "                print \"Done!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
