{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from q1_softmax import softmax\n",
    "# from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "# from q2_gradcheck import gradcheck_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input s should be the sigmoid\n",
    "    function value of your original input x.\n",
    "\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "\n",
    "    Return:a\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ds = s * (1 - s)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    \"\"\"\n",
    "    print(\"Checking gradient\")\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "    print(\"fx\")\n",
    "    print(fx)\n",
    "    print(\"grad\")\n",
    "    print(grad)\n",
    "    print(\"x\")\n",
    "    print(x)\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        # Try modifying x[ix] with h defined above to compute\n",
    "        # numerical gradients. Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later\n",
    "        random.setstate(rndstate)\n",
    "        x[ix] = x[ix] + 0.01*h\n",
    "        output, grad = f(x)\n",
    "#         print(\"output\")\n",
    "#         print(output)\n",
    "#         print(\"grad\")\n",
    "#         print(grad)\n",
    "        numgrad = (output - fx)/(0.01*h)\n",
    "#         print(\"numgrad:\")\n",
    "#         print(numgrad)\n",
    "#         print(\"grad[ix]\")\n",
    "#         print(grad[ix])\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad)\n",
    "#             return\n",
    "        x[ix] = x[ix] - 0.01*h\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Checking gradient\n",
      "fx\n",
      "74.8531346144\n",
      "grad\n",
      "[-0.79845108 -0.8851504   0.58965918  0.34971265  0.19731918  0.06283768\n",
      " -0.81923281 -0.28724546  0.28209052 -1.10470763  1.80542901  1.05187295\n",
      " -0.70504227  0.0681135   0.71720178  0.12728083 -1.24314054  0.46119353\n",
      " -0.47879999 -0.09539339 -0.22069002  1.11301291  0.72854775  0.6217782\n",
      " -0.22631176  0.91339399 -0.6206011   0.35716292  0.12496361  0.88945022\n",
      " -0.50780321 -0.67627295 -0.43746712  0.25756244  0.6446697  -0.67535046\n",
      " -0.58888323 -0.62716523  0.65041308 -0.25069872 -0.02554662  0.87787224\n",
      "  0.03690915 -1.02083075  0.38064863 -1.59096426 -0.36481116 -0.241267\n",
      " -0.22358285 -0.83097741  2.8142166   1.26214516  1.5616664  -1.01322161\n",
      "  1.66074571 -1.4771221   0.0633494  -0.19698928  0.51433955 -2.4011781\n",
      " -0.97610598 -0.67226468 -0.77105715  4.93645492  0.98057342 -0.72901054\n",
      "  0.04261211 -0.92003862  0.29290537 -1.8124683  -0.76081304 -0.78856735\n",
      " -0.02062642  3.84032514  0.85568165 -1.99239479  0.04609556 -0.96720058\n",
      "  0.46246579 -1.74553999 -0.96808232 -1.86415774  0.0236264   5.40290051\n",
      "  1.60228716 -2.574071    0.05378312 -0.22197074  0.30242699 -0.54783695\n",
      " -0.70042428 -0.77217929  0.02543953  3.50700234  0.92783029 -3.41506882\n",
      "  0.09239917 -0.10378571  0.13320169 -0.50670057 -0.98708086 -1.51528327\n",
      " -0.63211454  5.95693623  0.97749669 -3.93242191  0.14355379 -1.26825383\n",
      "  0.40246672 -2.66294612 -1.31633518 -1.58092663 -0.86326124  9.39567981\n",
      "  1.68244458]\n",
      "x\n",
      "[ 0.86795638 -0.69481048 -0.47322646  1.05019522  0.15872048 -0.42528556\n",
      "  0.26822306  0.90987821  1.11717477  0.6403198   0.64816788  0.3988415\n",
      "  0.35033416  0.5705386  -1.97643029 -1.9357476  -0.69478936  0.33504359\n",
      "  0.66399211  0.33397692  0.35013357 -0.37415816  0.4262603   0.60903649\n",
      " -0.22143287  0.18873784  0.3654935  -1.44692336 -0.00834208 -0.56451993\n",
      "  0.43355021 -0.9535835  -1.06737011 -0.02252714 -0.46128608 -1.96042527\n",
      "  0.74376828 -1.78217743 -0.64143622  0.91491797  0.19876704  0.12459236\n",
      "  0.06319215  0.96327378  0.79967233  0.41677499 -1.20054145 -0.68273762\n",
      "  1.65767387 -0.17611346 -0.36281642 -0.966615   -0.21377581 -0.33883652\n",
      "  1.74957057 -0.93696863  1.31481891 -0.39143498  1.18122718 -0.38702496\n",
      " -0.71753572  0.20275313  0.52699984  2.04702342  0.56051995  0.29733127\n",
      "  0.10573657  0.6460802  -0.22104895 -2.21295164 -0.94082448 -0.75263968\n",
      "  0.0766363   0.16772509  0.13211206 -1.38608193 -1.51834084 -0.03102379\n",
      " -0.38206958 -0.31860275  1.21569545 -0.41897107 -2.76726087  0.79786484\n",
      "  1.74600618 -0.1480546   0.02284158  0.84768799 -1.02888544 -1.21012953\n",
      "  0.38234898 -0.24940024 -1.57548934 -1.11936599 -0.49377867 -1.56480201\n",
      " -0.39019761  0.57636429  0.31124444 -0.01815265  0.39588747  0.51054466\n",
      " -0.17734067  0.90898682  1.76978401 -0.43985893 -1.10360348 -0.73088506\n",
      "  0.83807218  0.72532576  0.7583845   0.91371506 -0.2014515   1.25799758\n",
      " -0.55394255]\n",
      "Gradient check passed!\n",
      "Running your sanity checks...\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Arguments:\n",
    "    data -- M x Dx matrix, where each row is a training example.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    z1 = data.dot(W1) + b1\n",
    "    h1 = sigmoid(z1) \n",
    "    z2 = h1.dot(W2) + b2\n",
    "    output = softmax(z2) \n",
    "    cost = (-1) * np.sum(np.log(output) * labels)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    gradz2 = output - labels # equivalent to delta^(0) in notes\n",
    "    gradW2 = h1.T.dot(gradz2)\n",
    "    gradb2 = np.sum(gradz2, axis = 0)\n",
    "    \n",
    "    gradh1 = gradz2.dot(W2.T)\n",
    "    gradz1 = gradh1 * sigmoid_grad(h1) \n",
    "    gradW1 = data.T.dot(gradz1)\n",
    "    gradb1 = np.sum(gradz1, axis = 0)\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)\n",
    "\n",
    "\n",
    "def your_sanity_checks():\n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q2_neural.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print \"Running your sanity checks...\"\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
