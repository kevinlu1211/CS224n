{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. You might find numpy\n",
    "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
    "    broadcasting useful for this task.\n",
    "\n",
    "    Numpy broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    N-dimensional vector (treat the vector as a single row) and\n",
    "    for M x N matrices. This may be useful for testing later. Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the\n",
    "    written assignment!\n",
    "\n",
    "    Arguments:\n",
    "    x -- A N dimensional vector or M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        ### YOUR CODE HERE \n",
    "        max_vals = np.expand_dims(np.max(x, axis = 1), 0).T\n",
    "        x = x - max_vals\n",
    "        x = np.exp(x)\n",
    "        x_col_sums = np.expand_dims(np.sum(x, axis = 1), 0).T\n",
    "        x = x/x_col_sums\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "    else:\n",
    "        # Vector\n",
    "        ### YOUR CODE HERE\n",
    "        max_val = np.max(x)\n",
    "        x = x - max_val\n",
    "        x = np.exp(x)\n",
    "        x_sum = np.sum(x)\n",
    "        x = x/x_sum\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    ans1 = np.array([0.26894142,  0.73105858])\n",
    "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    ans3 = np.array([0.73105858, 0.26894142])\n",
    "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    print \"You should be able to verify these results by hand!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input s should be the sigmoid\n",
    "    function value of your original input x.\n",
    "\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "\n",
    "    Return:a\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ds = s * (1 - s)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print f\n",
    "    f_ans = np.array([\n",
    "        [0.73105858, 0.88079708],\n",
    "        [0.26894142, 0.11920292]])\n",
    "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
    "    print g\n",
    "    g_ans = np.array([\n",
    "        [0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])\n",
    "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "test_sigmoid_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    \"\"\"\n",
    "    print(\"Checking gradient\")\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "#     print(\"fx\")\n",
    "#     print(fx)\n",
    "#     print(\"grad\")\n",
    "#     print(grad)\n",
    "#     print(\"x\")\n",
    "#     print(x)\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        # Try modifying x[ix] with h defined above to compute\n",
    "        # numerical gradients. Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later\n",
    "        random.setstate(rndstate)\n",
    "        x[ix] = x[ix] + 0.01*h\n",
    "        output, grad = f(x)\n",
    "#         print(\"output\")\n",
    "#         print(output)\n",
    "#         print(\"grad\")\n",
    "#         print(grad)\n",
    "        numgrad = (output - fx)/(0.01*h)\n",
    "#         print(\"numgrad:\")\n",
    "#         print(numgrad)\n",
    "#         print(\"grad[ix]\")\n",
    "#         print(grad[ix])\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad)\n",
    "#             return\n",
    "        x[ix] = x[ix] - 0.01*h\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\"\n",
    "\n",
    "def test_gradcheck():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Checking gradient\n",
      "Gradient check passed!\n",
      "Checking gradient\n",
      "Gradient check passed!\n",
      "Checking gradient\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_gradcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Arguments:\n",
    "    data -- M x Dx matrix, where each row is a training example.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    z1 = data.dot(W1) + b1\n",
    "    h1 = sigmoid(z1) \n",
    "    z2 = h1.dot(W2) + b2\n",
    "    output = softmax(z2) \n",
    "    cost = (-1) * np.sum(np.log(output) * labels)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    gradz2 = output - labels # equivalent to delta^(0) in notes\n",
    "    gradW2 = h1.T.dot(gradz2)\n",
    "    gradb2 = np.sum(gradz2, axis = 0)\n",
    "    \n",
    "    gradh1 = gradz2.dot(W2.T)\n",
    "    gradz1 = gradh1 * sigmoid_grad(h1) \n",
    "    gradW1 = data.T.dot(gradz1)\n",
    "    gradb1 = np.sum(gradz1, axis = 0)\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def test_forward_backward_prop():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Checking gradient\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "test_forward_backward_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    row_sums = np.reshape((np.sum(x*x, axis = 1)), (x.shape[0], -1))\n",
    "    x = x/np.sqrt(row_sums)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]]))\n",
    "    print x\n",
    "    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_normalize_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "#     print(\"softmaxCostAndGradients\")\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    n, d = outputVectors.shape # n = number of samples, d = dimension of embedding space\n",
    "#     print(n, d)\n",
    "    prob = softmax(outputVectors.dot(predicted.T))\n",
    "#     print(prob)\n",
    "#     print(target)\n",
    "#     print(prob[target])\n",
    "    cost = -np.log(prob[target])\n",
    "    dx = prob\n",
    "    dx[target] -= 1\n",
    "#     print(outputVectors.T)\n",
    "#     print(dx)\n",
    "    gradPred = outputVectors.T.dot(dx.T)\n",
    "#     print(gradPred)\n",
    "    grad = np.reshape(dx, (n, 1)).dot(np.reshape(predicted, (1, d)))\n",
    "#     print(grad)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in xrange(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices\n",
    "\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "    \n",
    "    ### IMPLEMENTATION 1 (More readable, less efficient)###\n",
    "    \n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    n, d = outputVectors.shape # n = number of words, d = embedding space\n",
    "    targetVector = outputVectors[target]\n",
    "    scores = np.reshape(outputVectors.dot(predicted.T), (n, 1)) \n",
    "    \n",
    "    # Can use np.take\n",
    "#     cost = -np.log(sigmoid(scores[target])) - np.sum(np.log(sigmoid(-np.take(scores, indices[1:]))))\n",
    "\n",
    "    # But it's a bit overkill so let's just use array indexing\n",
    "    cost = -np.log(sigmoid(scores[target])) - np.sum(np.log(sigmoid(-scores[indices[1:]])))\n",
    "    gradPred = targetVector * (sigmoid(scores[target]) - 1) - \\\n",
    "                np.sum((sigmoid(-scores[indices[1:]]) - 1) * outputVectors[indices[1:]], axis = 0)\n",
    "   \n",
    "    # Add the gradients\n",
    "    for index in indices:\n",
    "        if index == target: \n",
    "            grad[index, :] += (sigmoid(scores[index]) - 1)*predicted\n",
    "        else:\n",
    "            grad[index, : ] += -(sigmoid(-scores[index]) -1)*predicted \n",
    "     \n",
    "    ### IMPLEMENTATION 2 from:\n",
    "    ### https://github.com/kvfrans/cs224-solutions/blob/master/a1/q3_word2vec.py\n",
    "    \n",
    "\n",
    "#     gradPred = np.zeros_like(predicted)\n",
    "#     grad = np.zeros_like(outputVectors)\n",
    "\n",
    "#     indices = [target]\n",
    "#     for k in xrange(K):\n",
    "#         newidx = dataset.sampleTokenIdx()\n",
    "#         while newidx == target:\n",
    "#             newidx = dataset.sampleTokenIdx()\n",
    "#         indices += [newidx]\n",
    "\n",
    "#     directions = np.array([1] + [-1 for k in xrange(K)])\n",
    "\n",
    "#     V = np.shape(outputVectors)[0]\n",
    "#     N = np.shape(outputVectors)[1]\n",
    "\n",
    "#     outputWords = outputVectors[indices,:]\n",
    "\n",
    "#     delta = sigmoid(np.dot(outputWords,predicted) * directions)\n",
    "#     deltaMinus = (delta - 1) * directions;\n",
    "#     cost = -np.sum(np.log(delta));\n",
    "\n",
    "#     gradPred = np.dot(deltaMinus.reshape(1,K+1),outputWords).flatten()\n",
    "#     gradMin = np.dot(deltaMinus.reshape(K+1,1),predicted.reshape(1,N))\n",
    "\n",
    "#     for k in xrange(K+1):\n",
    "#         grad[indices[k]] += gradMin[k,:]\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currrentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    currentWordIndex = tokens[currentWord]\n",
    "    currentWordVector = inputVectors[currentWordIndex]\n",
    "    for word in contextWords:\n",
    "        contextWordVector = outputVectors[tokens[word]]\n",
    "        inc_cost, inc_gradPred, inc_grad = word2vecCostAndGradient(currentWordVector, tokens[word], outputVectors, dataset)\n",
    "        cost += inc_cost\n",
    "        gradIn[currentWordIndex, :] += inc_gradPred\n",
    "        gradOut += inc_grad\n",
    "#     print(\"HIHI\")\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "\n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "\n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "\n",
    "    Extra credit: Implementing CBOW is optional, but the gradient\n",
    "    derivations are not. If you decide not to implement CBOW, remove\n",
    "    the NotImplementedError.\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n",
    "                         word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 1\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "\n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerword, C1, context, tokens, inputVectors, outputVectors,\n",
    "            dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "    \n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    return\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Checking gradient\n",
      "Gradient check passed!\n",
      "Checking gradient\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Checking gradient\n",
      "Gradient check passed!\n",
      "Checking gradient\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a cost and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD (also learning rate)\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    expcost = None\n",
    "\n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        # Don't forget to apply the postprocessing after every iteration!\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        cost, gradient = f(x)\n",
    "        x -= step * gradient\n",
    "        postprocessing(x)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print \"iter %d: %f\" % (iter, expcost)\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_sgd():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 1 result:\", t1\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 2 result:\", t2\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 3 result:\", t3\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.41483678608e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.52445103582e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingCostAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print \"sanity check: cost at convergence should be around or below 10\"\n",
    "print \"training took %d seconds\" % (time.time() - startTime)\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "# wordVectors = wordVectors[:nWords,:] + wordVectors[nWords:,:]\n",
    "\n",
    "visualizeWords = [\n",
    "    \"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\",\n",
    "    \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\",\n",
    "    \"annoying\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from utils.treebank import StanfordSentiment\n",
    "import utils.glove as glove\n",
    "\n",
    "from q3_sgd import load_saved_params, sgd\n",
    "\n",
    "# We will use sklearn here because it will run faster than implementing\n",
    "# ourselves. However, for other parts of this assignment you must implement\n",
    "# the functions yourself!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def getArguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--pretrained\", dest=\"pretrained\", action=\"store_true\",\n",
    "                       help=\"Use pretrained GloVe vectors.\")\n",
    "    group.add_argument(\"--yourvectors\", dest=\"yourvectors\", action=\"store_true\",\n",
    "                       help=\"Use your vectors from q3.\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def getSentenceFeatures(tokens, wordVectors, sentence):\n",
    "    \"\"\"\n",
    "    Obtain the sentence feature for sentiment analysis by averaging its\n",
    "    word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement computation for the sentence features given a sentence.\n",
    "\n",
    "    # Inputs:\n",
    "    # tokens -- a dictionary that maps words to their indices in\n",
    "    #           the word vector list\n",
    "    # wordVectors -- word vectors (each row) for all tokens\n",
    "    # sentence -- a list of words in the sentence of interest\n",
    "\n",
    "    # Output:\n",
    "    # - sentVector: feature vector for the sentence\n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    idx = []\n",
    "    for word in sentence:\n",
    "        idx.append(tokens[word])\n",
    "    sentVector = np.mean(wordVectors[idx,], axis = 0)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert sentVector.shape == (wordVectors.shape[1],)\n",
    "    return sentVector\n",
    "\n",
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    values = None   # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)\n",
    "\n",
    "\n",
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on parameter tuning on the dev set\n",
    "\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "    bestDevAccuracy = 0\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    for result in results:\n",
    "        if result[\"dev\"] > bestDevAccuracy:\n",
    "            bestDevAccuracy = result[\"dev\"]\n",
    "            bestResult = result\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return bestResult\n",
    "\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "\n",
    "def plotRegVsAccuracy(regValues, results, filename):\n",
    "    \"\"\" Make a plot of regularization vs accuracy \"\"\"\n",
    "    plt.plot(regValues, [x[\"train\"] for x in results])\n",
    "    plt.plot(regValues, [x[\"dev\"] for x in results])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"regularization\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(['train', 'dev'], loc='upper left')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputConfusionMatrix(features, labels, clf, filename):\n",
    "    \"\"\" Generate a confusion matrix \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    cm = confusion_matrix(labels, pred, labels=range(5))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.colorbar()\n",
    "    classes = [\"- -\", \"-\", \"neut\", \"+\", \"+ +\"]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputPredictions(dataset, features, labels, clf, filename):\n",
    "    \"\"\" Write the predictions to file \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    with open(filename, \"w\") as f:\n",
    "        print >> f, \"True\\tPredicted\\tText\"\n",
    "        for i in xrange(len(dataset)):\n",
    "            print >> f, \"%d\\t%d\\t%s\" % (\n",
    "                labels[i], pred[i], \" \".join(dataset[i][0]))\n",
    "\n",
    "def main(myVectors, pretrainedVectors):\n",
    "    \"\"\" Train a model to do sentiment analyis\"\"\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "\n",
    "    if myVectors:\n",
    "        _, wordVectors, _ = load_saved_params()\n",
    "        print(wordVectors.shape)\n",
    "        wordVectors = np.concatenate(\n",
    "            (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "            axis=1)\n",
    "        print(wordVectors.shape)\n",
    "    elif pretrainedVectors:\n",
    "        wordVectors = glove.loadWordVectors(tokens)\n",
    "        print(wordVectors.shape)\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "\n",
    "    # Load the train set\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "    for i in xrange(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "    for i in xrange(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare test set features\n",
    "    testset = dataset.getTestSentences()\n",
    "    nTest = len(testset)\n",
    "    testFeatures = np.zeros((nTest, dimVectors))\n",
    "    testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "    for i in xrange(nTest):\n",
    "        words, testLabels[i] = testset[i]\n",
    "        testFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # We will save our results from each run\n",
    "    results = []\n",
    "    regValues = getRegularizationValues()\n",
    "    for reg in regValues:\n",
    "        print \"Training for reg=%f\" % reg\n",
    "        # Note: add a very small number to regularization to please the library\n",
    "        clf = LogisticRegression(C=1.0/(reg + 1e-12))\n",
    "        clf.fit(trainFeatures, trainLabels)\n",
    "\n",
    "        # Test on train set\n",
    "        pred = clf.predict(trainFeatures)\n",
    "        trainAccuracy = accuracy(trainLabels, pred)\n",
    "        print \"Train accuracy (%%): %f\" % trainAccuracy\n",
    "\n",
    "        # Test on dev set\n",
    "        pred = clf.predict(devFeatures)\n",
    "        devAccuracy = accuracy(devLabels, pred)\n",
    "        print \"Dev accuracy (%%): %f\" % devAccuracy\n",
    "\n",
    "        # Test on test set\n",
    "        # Note: always running on test is poor style. Typically, you should\n",
    "        # do this only after validation.\n",
    "        pred = clf.predict(testFeatures)\n",
    "        testAccuracy = accuracy(testLabels, pred)\n",
    "        print \"Test accuracy (%%): %f\" % testAccuracy\n",
    "\n",
    "        results.append({\n",
    "            \"reg\": reg,\n",
    "            \"clf\": clf,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy})\n",
    "\n",
    "    # Print the accuracies\n",
    "    print \"\"\n",
    "    print \"=== Recap ===\"\n",
    "    print \"Reg\\t\\tTrain\\tDev\\tTest\"\n",
    "    for result in results:\n",
    "        print \"%.2E\\t%.3f\\t%.3f\\t%.3f\" % (\n",
    "            result[\"reg\"],\n",
    "            result[\"train\"],\n",
    "            result[\"dev\"],\n",
    "            result[\"test\"])\n",
    "    print \"\"\n",
    "\n",
    "    bestResult = chooseBestModel(results)\n",
    "    print \"Best regularization value: %0.2E\" % bestResult[\"reg\"]\n",
    "    print \"Test accuracy (%%): %f\" % bestResult[\"test\"]\n",
    "\n",
    "    # do some error analysis\n",
    "    if pretrainedVectors:\n",
    "        plotRegVsAccuracy(regValues, results, \"q4_reg_v_acc.png\")\n",
    "        outputConfusionMatrix(devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                              \"q4_dev_conf.png\")\n",
    "        outputPredictions(devset, devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                          \"q4_dev_pred.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39078, 10)\n",
      "(19539, 20)\n",
      "Training for reg=0.000001\n",
      "Train accuracy (%): 30.828652\n",
      "Dev accuracy (%): 29.609446\n",
      "Test accuracy (%): 29.638009\n",
      "Training for reg=0.000010\n",
      "Train accuracy (%): 30.828652\n",
      "Dev accuracy (%): 29.609446\n",
      "Test accuracy (%): 29.638009\n",
      "Training for reg=0.000100\n",
      "Train accuracy (%): 30.828652\n",
      "Dev accuracy (%): 29.609446\n",
      "Test accuracy (%): 29.638009\n",
      "Training for reg=0.001000\n",
      "Train accuracy (%): 30.828652\n",
      "Dev accuracy (%): 29.609446\n",
      "Test accuracy (%): 29.638009\n",
      "Training for reg=0.010000\n",
      "Train accuracy (%): 30.898876\n",
      "Dev accuracy (%): 29.609446\n",
      "Test accuracy (%): 29.683258\n",
      "Training for reg=0.100000\n",
      "Train accuracy (%): 30.770131\n",
      "Dev accuracy (%): 29.881926\n",
      "Test accuracy (%): 29.819005\n",
      "Training for reg=1.000000\n",
      "Train accuracy (%): 30.208333\n",
      "Dev accuracy (%): 30.426885\n",
      "Test accuracy (%): 29.411765\n",
      "Training for reg=10.000000\n",
      "Train accuracy (%): 29.283708\n",
      "Dev accuracy (%): 28.973660\n",
      "Test accuracy (%): 27.285068\n",
      "Training for reg=100.000000\n",
      "Train accuracy (%): 27.551498\n",
      "Dev accuracy (%): 25.976385\n",
      "Test accuracy (%): 23.755656\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\tDev\tTest\n",
      "1.00E-06\t30.829\t29.609\t29.638\n",
      "1.00E-05\t30.829\t29.609\t29.638\n",
      "1.00E-04\t30.829\t29.609\t29.638\n",
      "1.00E-03\t30.829\t29.609\t29.638\n",
      "1.00E-02\t30.899\t29.609\t29.683\n",
      "1.00E-01\t30.770\t29.882\t29.819\n",
      "1.00E+00\t30.208\t30.427\t29.412\n",
      "1.00E+01\t29.284\t28.974\t27.285\n",
      "1.00E+02\t27.551\t25.976\t23.756\n",
      "\n",
      "Best regularization value: 1.00E+00\n",
      "Test accuracy (%): 29.411765\n",
      "(19539, 50)\n",
      "Training for reg=0.000001\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.000010\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.877828\n",
      "Training for reg=0.000100\n",
      "Train accuracy (%): 39.934457\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.001000\n",
      "Train accuracy (%): 39.981273\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.010000\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.100000\n",
      "Train accuracy (%): 39.805712\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.149321\n",
      "Training for reg=1.000000\n",
      "Train accuracy (%): 39.524813\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=10.000000\n",
      "Train accuracy (%): 38.623596\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.692308\n",
      "Training for reg=100.000000\n",
      "Train accuracy (%): 36.329588\n",
      "Dev accuracy (%): 35.059037\n",
      "Test accuracy (%): 35.701357\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\tDev\tTest\n",
      "1.00E-06\t39.911\t36.421\t37.059\n",
      "1.00E-05\t39.923\t36.331\t36.878\n",
      "1.00E-04\t39.934\t36.331\t37.014\n",
      "1.00E-03\t39.981\t36.421\t36.968\n",
      "1.00E-02\t39.946\t36.331\t37.240\n",
      "1.00E-01\t39.806\t36.240\t37.149\n",
      "1.00E+00\t39.525\t36.603\t37.330\n",
      "1.00E+01\t38.624\t36.876\t37.692\n",
      "1.00E+02\t36.330\t35.059\t35.701\n",
      "\n",
      "Best regularization value: 1.00E+01\n",
      "Test accuracy (%): 37.692308\n"
     ]
    }
   ],
   "source": [
    "# Use our trained vectors\n",
    "main(True, False)\n",
    "\n",
    "# Use pretrained vectors\n",
    "main(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs224n]",
   "language": "python",
   "name": "conda-env-cs224n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
