{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. You might find numpy\n",
    "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
    "    broadcasting useful for this task.\n",
    "\n",
    "    Numpy broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    N-dimensional vector (treat the vector as a single row) and\n",
    "    for M x N matrices. This may be useful for testing later. Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the\n",
    "    written assignment!\n",
    "\n",
    "    Arguments:\n",
    "    x -- A N dimensional vector or M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        ### YOUR CODE HERE \n",
    "        max_vals = np.expand_dims(np.max(x, axis = 1), 0).T\n",
    "        x = x - max_vals\n",
    "        x = np.exp(x)\n",
    "        x_col_sums = np.expand_dims(np.sum(x, axis = 1), 0).T\n",
    "        x = x/x_col_sums\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "    else:\n",
    "        # Vector\n",
    "        ### YOUR CODE HERE\n",
    "        max_val = np.max(x)\n",
    "        x = x - max_val\n",
    "        x = np.exp(x)\n",
    "        x_sum = np.sum(x)\n",
    "        x = x/x_sum\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14483922,  0.1600721 ,  0.13105595,  0.09708864,  0.09708864,\n",
       "        0.13105595,  0.23879951])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([0.2, 0.3, 0.1, -0.2, -0.2, 0.1, 0.7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]]\n",
      "[[ 3.]\n",
      " [ 7.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a test matrix\n",
    "x = np.arange(8).reshape(2, 4).astype(np.float32)\n",
    "print(x)\n",
    "max_vals = np.expand_dims(np.max(x, axis = 1), 0).T\n",
    "print(max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3. -2. -1.  0.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Make the softmax numerically stable\n",
    "x = x - max_vals\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04978707  0.13533528  0.36787945  1.        ]\n",
      " [ 0.04978707  0.13533528  0.36787945  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.exp(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.55300176]\n",
      " [ 1.55300176]]\n"
     ]
    }
   ],
   "source": [
    "x_row_sums = np.expand_dims(np.sum(x, axis = 1), 0).T\n",
    "print(x_row_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0320586 ,  0.08714432,  0.23688284,  0.64391428],\n",
       "       [ 0.0320586 ,  0.08714432,  0.23688284,  0.64391428]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/x_row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    ans1 = np.array([0.26894142,  0.73105858])\n",
    "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    ans3 = np.array([0.73105858, 0.26894142])\n",
    "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    print \"You should be able to verify these results by hand!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_softmax():\n",
    "    \"\"\"\n",
    "    Use this space to test your softmax implementation by running:\n",
    "        python q1_softmax.py\n",
    "    This function will not be called by the autograder, nor will\n",
    "    your tests be graded.\n",
    "    \"\"\"\n",
    "    print \"Running your tests...\"\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
